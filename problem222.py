# -*- coding: utf-8 -*-
"""problem222.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x1VvqWgsRojm2GE7LWY05pvHMj9VIBM-
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import time, random
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC

def name_to_seed(name="Sai Charan"):
    return sum(ord(c) for c in name)

GLOBAL_SEED = name_to_seed("Sai Charan")  # e.g., 992 → deterministic seed
np.random.seed(GLOBAL_SEED)
random.seed(GLOBAL_SEED)
tf.random.set_seed(GLOBAL_SEED)

# Config
IMG_H, IMG_W = 28, 28
IMG_SIZE = IMG_H * IMG_W
N_CLASSES = 10

OPTIMIZER_NAME = "adam"
LR = 1e-3
BATCH_SIZE = 128
EPOCHS = 10

DO_TSNE_KMEANS = True
DO_SKLEARN_BASELINES = True

# Data: Fashion-MNIST (55k train, 5k val, 10k test)
def _prep(ex):
    x = tf.cast(ex['image'], tf.float32) / 255.0
    x = tf.reshape(x, [IMG_SIZE])
    y = tf.one_hot(tf.cast(ex['label'], tf.int32), depth=N_CLASSES)
    return x, y

ds_train_all, ds_test = tfds.load(
    "fashion_mnist", split=["train", "test"], as_supervised=False, shuffle_files=True
)

ds_train = ds_train_all.take(55_000).map(_prep, num_parallel_calls=tf.data.AUTOTUNE)
ds_val   = ds_train_all.skip(55_000).take(5_000).map(_prep, num_parallel_calls=tf.data.AUTOTUNE)
ds_test  = ds_test.map(_prep, num_parallel_calls=tf.data.AUTOTUNE)

ds_train = ds_train.cache()
ds_val   = ds_val.cache()
ds_test  = ds_test.cache()

def make_ds(ds, bs, training=True):
    if training:
        ds = ds.shuffle(4096, seed=GLOBAL_SEED, reshuffle_each_iteration=True)
    return ds.batch(bs).prefetch(tf.data.AUTOTUNE)

train_ds = make_ds(ds_train, BATCH_SIZE, training=True)
val_ds   = make_ds(ds_val,   BATCH_SIZE, training=False)
test_ds  = make_ds(ds_test,  BATCH_SIZE, training=False)

print(f"Sizes -> train: (55000, 784) val: (5000, 784) test: (10000, 784)")

# Model: Logistic Regression (TensorFlow Variables)
class LogisticRegressionTF:
    def __init__(self, in_dim, n_classes, seed=GLOBAL_SEED):
        self.W = tf.Variable(tf.random.normal([in_dim, n_classes], stddev=0.01, seed=seed))
        self.b = tf.Variable(tf.zeros([n_classes], dtype=tf.float32))

    def logits(self, X):  return tf.matmul(X, self.W) + self.b
    def probs(self, X):   return tf.nn.softmax(self.logits(X), axis=1)

def loss_fn(y_true, logits):
    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=logits))

def get_optimizer(name, lr):
    n = name.lower()
    if n == "sgd":     return tf.optimizers.SGD(lr)
    if n == "adam":    return tf.optimizers.Adam(lr)
    if n == "rmsprop": return tf.optimizers.RMSprop(lr)
    raise ValueError("Unknown optimizer")

def accuracy_on_dataset(model, dataset):
    correct, total = 0, 0
    for xb, yb in dataset:
        preds = tf.argmax(model.probs(xb), axis=1)
        labs  = tf.argmax(yb,            axis=1)
        correct += tf.reduce_sum(tf.cast(tf.equal(preds, labs), tf.int32)).numpy()
        total   += xb.shape[0]
    return correct / total

def loss_on_dataset(model, dataset):
    vals = []
    for xb, yb in dataset:
        vals.append(loss_fn(yb, model.logits(xb)).numpy())
    return float(np.mean(vals))

# Training loop
def train():
    model = LogisticRegressionTF(IMG_SIZE, N_CLASSES)
    opt   = get_optimizer(OPTIMIZER_NAME, LR)
    tr_acc_hist, va_acc_hist, tr_loss_hist, va_loss_hist, times = [], [], [], [], []

    for ep in range(1, EPOCHS + 1):
        t0 = time.time()
        for xb, yb in train_ds:
            with tf.GradientTape() as tape:
                logits = model.logits(xb)
                loss   = loss_fn(yb, logits)
            dW, db = tape.gradient(loss, [model.W, model.b])
            (dW, db), _ = tf.clip_by_global_norm([dW, db], 5.0)
            opt.apply_gradients([(dW, model.W), (db, model.b)])
        dur = time.time() - t0

        tr_acc = accuracy_on_dataset(model, train_ds)
        va_acc = accuracy_on_dataset(model, val_ds)
        tr_loss = loss_on_dataset(model, train_ds)
        va_loss = loss_on_dataset(model, val_ds)

        tr_acc_hist.append(tr_acc); va_acc_hist.append(va_acc)
        tr_loss_hist.append(tr_loss); va_loss_hist.append(va_loss); times.append(dur)

        print(f"Epoch {ep:02d}/{EPOCHS:02d} - time {dur:0.2f}s | "
              f"train_loss={tr_loss:0.4f} train_acc={tr_acc:0.4f} | "
              f"val_loss={va_loss:0.4f} val_acc={va_acc:0.4f}")

    return model, (tr_acc_hist, va_acc_hist, tr_loss_hist, va_loss_hist, times)

print("\n== Training ==")
model, (train_acc_hist, val_acc_hist, train_loss_hist, val_loss_hist, times) = train()


# Test Evaluation
test_acc = accuracy_on_dataset(model, test_ds)
print(f"\nFinal Test Accuracy: {test_acc:.4f}")

y_true_all, y_pred_all = [], []
for xb, yb in test_ds:
    p = model.probs(xb).numpy()
    y_pred_all.append(np.argmax(p, axis=1))
    y_true_all.append(np.argmax(yb.numpy(), axis=1))
y_true_all = np.concatenate(y_true_all)
y_pred_all = np.concatenate(y_pred_all)

cm = confusion_matrix(y_true_all, y_pred_all)
print("\nConfusion matrix (test):")
print(cm)
print("\n" + classification_report(y_true_all, y_pred_all, digits=4))


# Training curves (Accuracy)
plt.figure(figsize=(6,4))
plt.plot(train_acc_hist, label="train_acc")
plt.plot(val_acc_hist,   label="val_acc")
plt.title("Training and validation accuracy/loss curves.")
plt.xlabel("Epoch"); plt.legend(); plt.tight_layout(); plt.show()

# 3×3 sample predictions
def plot_images(images, y, yhat=None):
    assert len(images) == len(y) == 9
    fig, axes = plt.subplots(3, 3, figsize=(6, 6))
    fig.subplots_adjust(hspace=0.3, wspace=0.3)
    for i, ax in enumerate(axes.flat):
        ax.imshow(images[i].reshape(IMG_H, IMG_W), cmap='binary')
        label = f"True: {y[i]}" if yhat is None else f"True: {y[i]}, Pred: {yhat[i]}"
        ax.set_xlabel(label); ax.set_xticks([]); ax.set_yticks([])
    plt.show()

sample_imgs, sample_lbls = next(iter(test_ds))
sample_imgs = sample_imgs.numpy()
true_idx    = np.argmax(sample_lbls.numpy(), axis=1)
pred_idx    = np.argmax(model.probs(sample_imgs).numpy(), axis=1)
plot_images(sample_imgs[:9], true_idx[:9], pred_idx[:9])

# 3×4 Weights Heatmaps
def plot_weights_grid(W):
    Wnp = W.numpy()
    w_min, w_max = Wnp.min(), Wnp.max()
    fig, axes = plt.subplots(3, 4, figsize=(8,6))
    fig.subplots_adjust(hspace=0.3, wspace=0.3)
    for i, ax in enumerate(axes.flat):
        if i < 10:
            ax.imshow(Wnp[:, i].reshape(IMG_H, IMG_W), vmin=w_min, vmax=w_max, cmap='seismic')
            ax.set_xlabel(f"Weights: {i}")
        ax.set_xticks([]); ax.set_yticks([])
    plt.show()

plot_weights_grid(model.W)

# KMeans + t-SNE Visualization
if DO_TSNE_KMEANS:
    try:
        Wcols = model.W.numpy().T  # (10, 784)
        kmeans = KMeans(n_clusters=3, random_state=GLOBAL_SEED, n_init="auto").fit(Wcols)
        print("KMeans cluster labels for classes:", kmeans.labels_.tolist())

        tsne = TSNE(n_components=2, random_state=GLOBAL_SEED, init="pca", perplexity=3)
        W2d = tsne.fit_transform(Wcols)

        plt.figure(figsize=(6,5))
        for i in range(10):
            plt.scatter(W2d[i,0], W2d[i,1], s=80)
            plt.text(W2d[i,0]+0.8, W2d[i,1]+0.8, str(i), fontsize=9)
        plt.title("t-SNE of class-weight vectors (perplexity=3)")
        plt.tight_layout(); plt.show()
    except Exception as e:
        print("t-SNE / KMeans skipped due to error:", e)

if DO_SKLEARN_BASELINES:
    try:
        print("Training sklearn models on subset (this may take a while)...")
        k = 2000
        X_small, y_small = [], []
        for xb, yb in test_ds:
            X_small.append(xb.numpy()); y_small.append(np.argmax(yb.numpy(), axis=1))
            if sum(len(y) for y in y_small) >= k:
                break
        X_small = np.vstack(X_small)[:k]
        y_small = np.concatenate(y_small)[:k]

        rf = RandomForestClassifier(n_estimators=200, random_state=GLOBAL_SEED, n_jobs=-1)
        rf.fit(X_small, y_small)
        print("RandomForest (subset) test acc (first 2k):", rf.score(X_small, y_small))

        svm = LinearSVC(C=1.0, max_iter=3000, dual=False, random_state=GLOBAL_SEED)
        svm.fit(X_small, y_small)
        print("SVM (subset) test acc (first 2k):", svm.score(X_small, y_small))
    except Exception as e:
        print("Sklearn baselines skipped:", e)