# -*- coding: utf-8 -*-
"""problem 111

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Vkjqvg_J04py6z7Dy5zjJUpNflQKdGZ
"""

import math, time, string
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt


def name_to_seed(name="Sai Charan"):
    return sum(ord(c) for c in name)

GLOBAL_SEED = name_to_seed("Sai Charan")   # e.g., 856 (sum of ASCII values)
tf.keras.utils.set_random_seed(GLOBAL_SEED)  # sets TF + NumPy + Python RNGs


NUM_EXAMPLES      = 500
TRUE_W, TRUE_B    = 3.0, 2.0
TRAIN_STEPS       = 1000
BASE_LR           = 1e-3
LOSS_CHOICE       = "hybrid"   # "mse" | "huber" | "hybrid"
HUBER_DELTA       = 1.0

# Patience schedule: if |Î”loss| < 1e-6 for 20 consecutive steps -> lr /= 2
DELTA_EPS         = 1e-6
PATIENCE_STEPS    = 20

# Noise controls
DATA_NOISE_STD    = 1.0    # Gaussian noise on outputs y
WEIGHT_NOISE_STD  = 0.0    # per-step Gaussian noise on weights
LR_NOISE_STD      = 0.0    # per-step multiplicative noise on LR (e.g., 0.01 adds ~1% jitter)


# Data
g = tf.random.Generator.from_seed(GLOBAL_SEED)
X = g.normal([NUM_EXAMPLES], dtype=tf.float32)
noise = g.normal([NUM_EXAMPLES], dtype=tf.float32) * DATA_NOISE_STD
y = TRUE_W * X + TRUE_B + noise

# Trainable variables
W = tf.Variable(0.0, dtype=tf.float32)
b = tf.Variable(0.0, dtype=tf.float32)

# Prediction
def prediction(x):
    return W * x + b

# Loss functions
mse = tf.keras.losses.MeanSquaredError(reduction="sum_over_batch_size")

def squared_loss(y_true, y_pred):
    return mse(y_true, y_pred)

def huber_loss(y_true, y_pred, delta=HUBER_DELTA):
    return tf.reduce_mean(tf.keras.losses.huber(y_true, y_pred, delta=delta))

def hybrid_loss(y_true, y_pred):
    # L2 (MSE) + L1 (MAE)
    l2 = squared_loss(y_true, y_pred)
    l1 = tf.reduce_mean(tf.abs(y_true - y_pred))
    return l2 + l1

LOSS_FN = {
    "mse": squared_loss,
    "huber": lambda yt, yp: huber_loss(yt, yp, HUBER_DELTA),
    "hybrid": hybrid_loss
}[LOSS_CHOICE]

# Optimizer (SGD for manual LR control)
optimizer = tf.keras.optimizers.SGD(learning_rate=BASE_LR, momentum=0.0, nesterov=False)

# Training utilities
loss_history = []
stable_count = 0
prev_loss = None

def maybe_decay_lr(curr_loss):
    global stable_count, prev_loss
    if prev_loss is not None and abs(curr_loss - prev_loss) < DELTA_EPS:
        stable_count += 1
        if stable_count >= PATIENCE_STEPS:
            new_lr = float(optimizer.learning_rate) * 0.5
            optimizer.learning_rate = new_lr
            stable_count = 0
    else:
        stable_count = 0
    prev_loss = curr_loss

def inject_weight_noise():
    if WEIGHT_NOISE_STD > 0.0:
        W.assign_add(tf.random.normal(shape=[], stddev=WEIGHT_NOISE_STD, seed=GLOBAL_SEED))
        b.assign_add(tf.random.normal(shape=[], stddev=WEIGHT_NOISE_STD, seed=GLOBAL_SEED + 1))

def lr_with_noise():
    base_lr = float(optimizer.learning_rate)
    if LR_NOISE_STD > 0.0:
        jitter = np.random.normal(loc=0.0, scale=LR_NOISE_STD)
        return max(1e-8, base_lr * (1.0 + jitter))
    return base_lr

# Training loop
start_time = time.time()
for step in range(1, TRAIN_STEPS + 1):
    inject_weight_noise()

    with tf.GradientTape() as tape:
        y_pred = prediction(X)
        loss = LOSS_FN(y, y_pred)

    grads = tape.gradient(loss, [W, b])

    current_lr = lr_with_noise()
    optimizer.learning_rate = current_lr
    optimizer.apply_gradients(zip(grads, [W, b]))

    curr_loss = float(loss.numpy())
    loss_history.append(curr_loss)
    maybe_decay_lr(curr_loss)

end_time = time.time()

# Results
print(f"Seed (ASCII sum of 'Sai Charan'): {GLOBAL_SEED}")
print(f"Final loss ({LOSS_CHOICE}): {loss_history[-1]:.6f}")
print(f"Learned parameters: W = {W.numpy():.4f}, b = {b.numpy():.4f}")
print(f"Initial LR = {BASE_LR}, Final LR = {float(optimizer.learning_rate):.6g}")
print(f"Training steps = {TRAIN_STEPS}, Duration = {end_time - start_time:.2f}s")

# Plot results
plt.figure(figsize=(6,4))
plt.plot(X.numpy(), y.numpy(), 'bo', alpha=0.4, label='Noisy data')
xline = tf.linspace(tf.reduce_min(X)-0.5, tf.reduce_max(X)+0.5, 200)
yline = prediction(xline)
plt.plot(xline.numpy(), yline.numpy(), 'r', linewidth=2,
         label=f"{LOSS_CHOICE.capitalize()} fit: y = {W.numpy():.2f}x + {b.numpy():.2f}")
plt.title("Linear Regression with Noise & Patience LR Scheduling (Sai Charan)")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.tight_layout()
plt.show()

# Plot loss curve
plt.figure(figsize=(6,3))
plt.plot(loss_history)
plt.title("Training Loss (Sai Charan Seed)")
plt.xlabel("Step")
plt.ylabel("Loss")
plt.tight_layout()
plt.show()